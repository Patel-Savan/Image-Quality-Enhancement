{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece6fc2-33b5-4fa8-aa13-9de96f91be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib numpy opencv-python tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34554a-5772-43b7-b01e-ac35e386f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04954b-3d78-4edb-9067-91c199efd10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "dataset_path = r\"C:\\Users\\ASUS\\Desktop\\University\\sem-4\\FER2013\"\n",
    "train_path = os.path.join(dataset_path, \"train\")\n",
    "test_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "# Define noisy dataset paths\n",
    "noisy_dataset_path = r\"C:\\Users\\ASUS\\Desktop\\University\\sem-4\\FER2013_Noisy_Images\"\n",
    "noisy_train_path = os.path.join(noisy_dataset_path, \"train\")\n",
    "noisy_test_path = os.path.join(noisy_dataset_path, \"test\")\n",
    "\n",
    "# Ensure noisy dataset directories exist\n",
    "for path in [noisy_train_path, noisy_test_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc3bec-a9be-46d1-a546-f71504e29c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(folder_path, num_images=5):\n",
    "    categories = os.listdir(folder_path)\n",
    "    fig, axes = plt.subplots(len(categories), num_images, figsize=(10, 10))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "        image_files = os.listdir(category_path)[:num_images]\n",
    "\n",
    "        for j, image_file in enumerate(image_files):\n",
    "            img_path = os.path.join(category_path, image_file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) \n",
    "            img = cv2.resize(img, (48, 48)) \n",
    "\n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].axis(\"off\")\n",
    "\n",
    "        axes[i, 0].set_ylabel(category, fontsize=12)\n",
    "\n",
    "    # category_path = os.path.join(folder_path, \"neutral\")\n",
    "    # image_files = os.listdir(category_path)[:num_images]\n",
    "\n",
    "    # for j, image_file in enumerate(image_files):\n",
    "    #     img_path = os.path.join(category_path, image_file)\n",
    "    #     img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) \n",
    "    #     img = cv2.resize(img, (48, 48)) \n",
    "        \n",
    "    #     axes[j].imshow(img, cmap='gray')\n",
    "    #     axes[j].axis(\"off\")\n",
    "\n",
    "    # axes[0].set_ylabel(\"neutral\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(train_path, num_images=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce31c2e-4782-474f-97ff-fef75f74575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(image, mean=0, std=1):  \n",
    "    noise = np.random.normal(mean, std, image.shape).astype(np.uint8)\n",
    "    noisy_image = cv2.add(image, noise)  # Add noise to the image\n",
    "    return noisy_image\n",
    "\n",
    "# Function to process images and save noisy versions\n",
    "def process_images(input_folder, output_folder, category=\"neutral\"):\n",
    "    category_path = os.path.join(input_folder, category)\n",
    "    noisy_category_path = os.path.join(output_folder, category)\n",
    "\n",
    "    # Ensure category subfolder exists in noisy images\n",
    "    if not os.path.exists(noisy_category_path):\n",
    "        os.makedirs(noisy_category_path)\n",
    "\n",
    "    # Process images inside category folder\n",
    "    for img_name in tqdm(os.listdir(category_path), desc=f\"Processing {category} in {output_folder}\"):\n",
    "        img_path = os.path.join(category_path, img_name)\n",
    "        noisy_img_path = os.path.join(noisy_category_path, img_name)\n",
    "\n",
    "        # Read and process image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            continue  # Skip unreadable images\n",
    "            \n",
    "        noisy_image = add_gaussian_noise(image)\n",
    "\n",
    "        # Save noisy image\n",
    "        cv2.imwrite(noisy_img_path, noisy_image)\n",
    "\n",
    "# Apply noise to both train and test datasets for neutral category\n",
    "process_images(train_path, noisy_train_path, category=\"neutral\")\n",
    "process_images(test_path, noisy_test_path, category=\"neutral\")\n",
    "\n",
    "print(\"âœ… Noisy images successfully generated and saved in 'FER2013_Noisy_Images' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca216e7-9656-4820-93f4-6140541e42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_original_and_noisy_images(original_folder, noisy_folder, num_images=5, category=\"neutral\"):\n",
    "    original_category_path = os.path.join(original_folder, category)\n",
    "    noisy_category_path = os.path.join(noisy_folder, category)\n",
    "\n",
    "    if not os.path.exists(noisy_category_path):\n",
    "        print(f\"Skipping {category}, noisy folder not found.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images * 2, figsize=(15, 5))\n",
    "\n",
    "    image_files = os.listdir(original_category_path)[:num_images]\n",
    "\n",
    "    for j, image_file in enumerate(image_files):\n",
    "        original_img_path = os.path.join(original_category_path, image_file)\n",
    "        noisy_img_path = os.path.join(noisy_category_path, image_file)\n",
    "\n",
    "        original_img = cv2.imread(original_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        noisy_img = cv2.imread(noisy_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if original_img is None or noisy_img is None:\n",
    "            continue  # Skip if images are unreadable\n",
    "\n",
    "        original_img = cv2.resize(original_img, (48, 48))\n",
    "        noisy_img = cv2.resize(noisy_img, (48, 48))\n",
    "\n",
    "        # Show original image\n",
    "        axes[j * 2].imshow(original_img, cmap='gray')\n",
    "        axes[j * 2].axis(\"off\")\n",
    "        axes[j * 2].set_title(\"Original\")\n",
    "\n",
    "        # Show noisy image\n",
    "        axes[j * 2 + 1].imshow(noisy_img, cmap='gray')\n",
    "        axes[j * 2 + 1].axis(\"off\")\n",
    "        axes[j * 2 + 1].set_title(\"Noisy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display original and noisy images for neutral category\n",
    "display_original_and_noisy_images(train_path, noisy_train_path, num_images=5, category=\"neutral\")\n",
    "display_original_and_noisy_images(test_path, noisy_test_path, num_images=5, category=\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adfe32-b2f8-4a44-824d-6a493cad3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path, category=\"neutral\", image_size=(48, 48)):\n",
    "    data = []\n",
    "    category_path = os.path.join(folder_path, category)\n",
    "    for img_name in os.listdir(category_path):\n",
    "        img_path = os.path.join(category_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, image_size)\n",
    "        data.append(img)\n",
    "    return np.array(data).astype(\"float32\") / 255.0  # Normalize images\n",
    "\n",
    "# Load noisy and clean images for the \"neutral\" category\n",
    "X_noisy = load_images(noisy_train_path, category=\"neutral\")\n",
    "X_clean = load_images(train_path, category=\"neutral\")\n",
    "\n",
    "print(\"Images Loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155c01b-94af-438c-ad83-d3f23d09cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dae():\n",
    "    input_img = tf.keras.Input(shape=(48, 48, 1))\n",
    "\n",
    "    # Encoder \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(input_img)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "    # Decoder \n",
    "    x = tf.keras.layers.Conv2DTranspose(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    output_img = tf.keras.layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    model = tf.keras.Model(input_img, output_img)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the DAE model\n",
    "dae = build_dae()\n",
    "dae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a89e1-067f-4d07-bc5e-ed6e220378b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "# Build the model once\n",
    "dae = build_dae()\n",
    "initial_weights = dae.get_weights()  # Store initial weights\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_noisy):\n",
    "    X_train_fold, X_val_fold = X_noisy[train_idx], X_noisy[val_idx]\n",
    "    y_train_fold, y_val_fold = X_clean[train_idx], X_clean[val_idx]\n",
    "\n",
    "    dae.set_weights(initial_weights)  # Reset weights instead of re-building\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\"temp_best_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0)\n",
    "    history = dae.fit(X_train_fold, y_train_fold, epochs=20, batch_size=16,\n",
    "                      validation_data=(X_val_fold, y_val_fold), callbacks=[checkpoint], verbose=0)\n",
    "\n",
    "    dae.load_weights(\"temp_best_model.keras\")  # Load best weights for this fold\n",
    "    val_loss = min(history.history[\"val_loss\"])\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        dae.save(\"best_dae_model.keras\")  # Save the best model\n",
    "\n",
    "print(\"Best model saved as 'best_dae_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37532939-6d80-4bb5-b2a1-39b38a712815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "# Performance Metrics Calculation (MSE, PSNR, SSIM)\n",
    "def calculate_metrics(original, denoised):\n",
    "    metrics = {\"MSE\": [], \"PSNR\": [], \"SSIM\": []}\n",
    "\n",
    "    for i in range(original.shape[0]):\n",
    "        orig, denoised_img = original[i].squeeze(), denoised[i].squeeze()\n",
    "\n",
    "        # Mean Squared Error\n",
    "        mse_value = np.mean((orig - denoised_img) ** 2)\n",
    "        metrics[\"MSE\"].append(mse_value)\n",
    "\n",
    "        # Peak Signal-to-Noise Ratio\n",
    "        psnr_value = psnr(orig, denoised_img, data_range=orig.max() - orig.min())\n",
    "        metrics[\"PSNR\"].append(psnr_value)\n",
    "\n",
    "        # Structural Similarity Index\n",
    "        ssim_value = ssim(orig, denoised_img, data_range=orig.max() - orig.min())\n",
    "        metrics[\"SSIM\"].append(ssim_value)\n",
    "\n",
    "    # Compute mean values for each metric\n",
    "    return {key: np.mean(values) for key, values in metrics.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad28d9-54fe-4c56-9bb6-1310bd454a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(\"best_dae_model.keras\")\n",
    "\n",
    "# Load 20% unseen test data\n",
    "X_unseen = load_images(noisy_test_path, category = \"neutral\")\n",
    "y_unseen = load_images(test_path, category = \"neutral\")\n",
    "\n",
    "# Predict denoised images using the trained model\n",
    "denoised_images = best_model.predict(X_unseen)\n",
    "\n",
    "# Calculate metrics (MSE, PSNR, SSIM)\n",
    "unseen_metrics = calculate_metrics(y_unseen, denoised_images)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance on Unseen Data:\")\n",
    "for metric, value in unseen_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528edb5d-fcf2-4b22-be18-e66cf4afa323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save denoised images\n",
    "denoised_images_dir = r\"C:\\Users\\ASUS\\Desktop\\University\\sem-4\\FER2013_Denoised_Images\"\n",
    "neutral_denoised_images = os.path.join(denoised_images_dir, \"neutral\")\n",
    "\n",
    "if not os.path.exists(neutral_denoised_images):\n",
    "    os.makedirs(neutral_denoised_images)\n",
    "\n",
    "# Save denoised images\n",
    "for i in range(len(denoised_images)):\n",
    "    denoised_img = denoised_images[i].squeeze()\n",
    "    denoised_img_path = os.path.join(neutral_denoised_images, f\"denoised_{i+1}.png\")\n",
    "    plt.imsave(denoised_img_path, denoised_img, cmap='gray')\n",
    "\n",
    "print(f\"Denoised images saved to {denoised_images_dir} directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f487e34-4972-4d03-ac30-31fe4b4f2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of images to display\n",
    "num_images_to_display = 6\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(num_images_to_display):\n",
    "    # Original clean image\n",
    "    plt.subplot(3, num_images_to_display, i + 1)\n",
    "    plt.imshow(y_unseen[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    # Noisy image\n",
    "    plt.subplot(3, num_images_to_display, i + num_images_to_display + 1)\n",
    "    plt.imshow(X_unseen[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Noisy\")\n",
    "\n",
    "    # Denoised image\n",
    "    plt.subplot(3, num_images_to_display, i + 2 * num_images_to_display + 1)\n",
    "    plt.imshow(denoised_images[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Denoised\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54a118-a01f-496c-9150-ab1a03a86318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
